import requests
from bs4 import BeautifulSoup
import time
import re

BASE_URL = "https://tubitak.gov.tr"
LIST_URL = f"{BASE_URL}/tr/destekler/sanayi/ulusal-destek-programlari"

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}


def clean_text(text):
    """Gereksiz boÅŸluklarÄ± ve satÄ±r sonlarÄ±nÄ± temizler"""
    text = re.sub(r"\s+", " ", text).strip()
    return text


def get_call_links_and_names():
    """Liste sayfasÄ±ndaki Ã§aÄŸrÄ± adlarÄ±nÄ± ve linklerini dÃ¶ndÃ¼rÃ¼r"""
    r = requests.get(LIST_URL, headers=headers)
    soup = BeautifulSoup(r.text, "html.parser")

    container = soup.select_one("#paragraph-id--311 > div > div > div > div")
    if not container:
        print("âš ï¸ Ã‡aÄŸrÄ±lar container'Ä± bulunamadÄ±.")
        return []

    calls = []
    seen_urls = set()  # Tekrar eden URL'leri Ã¶nlemek iÃ§in

    for div in container.select("div > div > div > div > div"):
        a = div.find("a")
        if a and "href" in a.attrs:
            name = clean_text(a.get_text())
            link = a["href"]
            if not link.startswith("http"):
                link = BASE_URL + link

            # AynÄ± URL'yi tekrar ekleme
            if link not in seen_urls:
                seen_urls.add(link)
                calls.append({"name": name, "url": link})

    return calls


def get_applicant_info(url):
    """Ã‡aÄŸrÄ± detay sayfasÄ±ndan yalnÄ±zca 'Kimler BaÅŸvurabilir' kÄ±smÄ±nÄ± dÃ¶ndÃ¼rÃ¼r"""
    r = requests.get(url, headers=headers)
    soup = BeautifulSoup(r.text, "html.parser")

    # "Kimler BaÅŸvurabilir" baÅŸlÄ±ÄŸÄ±nÄ± bul
    basliklar = soup.select(".field--name-field-baslik.field__item")

    for baslik in basliklar:
        if "Kimler BaÅŸvurabilir" in baslik.get_text(strip=True):
            # Parent paragraph'Ä± bul
            parent_paragraph = baslik.find_parent("div", class_="paragraph")
            if parent_paragraph:
                # Ä°Ã§erik alanÄ±nÄ± bul
                icerik = parent_paragraph.select_one(".field--name-field-icerik.field__item")
                if not icerik:
                    return None

                # TÃ¼m iÃ§eriÄŸi birleÅŸtir - hem p hem de ul/li elementlerini
                full_text_parts = []

                # Ã–nce paragraflarÄ± al
                for p in icerik.find_all("p"):
                    text = clean_text(p.get_text())
                    if text:
                        full_text_parts.append(text)

                # Sonra liste elemanlarÄ±nÄ± al
                for li in icerik.find_all("li"):
                    text = clean_text(li.get_text())
                    if text:
                        full_text_parts.append(text)

                # EÄŸer hiÃ§bir ÅŸey bulunamadÄ±ysa, tÃ¼m iÃ§eriÄŸi al
                if not full_text_parts:
                    full_text = clean_text(icerik.get_text())
                    if full_text:
                        full_text_parts = [full_text]

                # TÃ¼m parÃ§alarÄ± birleÅŸtir
                if full_text_parts:
                    return [" ".join(full_text_parts)]

                return None
    return None


def main():
    calls = get_call_links_and_names()
    print(f"ğŸ”— {len(calls)} Ã§aÄŸrÄ± bulundu.\n")

    for i, call in enumerate(calls, 1):
        print(f"[{i}/{len(calls)}] {call['name']}")
        print("=" * 80)
        try:
            maddeler = get_applicant_info(call["url"])
            if maddeler:
                print("Kimler BaÅŸvurabilir:")
                for madde in maddeler:
                    print(f"  {madde}")
            else:
                print("(veri bulunamadÄ±)")
        except Exception as e:
            print(f"âš ï¸ Hata: {e}")

        print("\n" + "-" * 80 + "\n")
        time.sleep(2)

    print("âœ… TÃ¼m Ã§aÄŸrÄ±lar iÅŸlendi.")


if __name__ == "__main__":
    main()
